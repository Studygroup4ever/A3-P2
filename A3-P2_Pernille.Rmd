---
title: "Assignment 3 - Part 2 - Diagnosing Schizophrenia from Voice"
author: "Riccardo Fusaroli"
date: "October 17, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Assignment 3 - Part 2 - Diagnosing schizophrenia from voice

In the previous part of the assignment you generated a bunch of "features", that is, of *quantitative descriptors* (fx IQR, pause_dur..) of voice in schizophrenia. We then looked at whether we could replicate results from the previous literature.
We now want to know whether we can automatically diagnose schizophrenia from voice only, that is, relying on the set of features you produced last time, we will try to produce an automated classifier.
Again, remember that the dataset containst 7 studies and 3 languages. Feel free to only include Danish (Study 1-4) if you feel that adds too much complexity.

Issues to be discussed your report:
- Should you run the analysis on all languages/studies at the same time? 
- Choose your best acoustic feature from part 1. How well can you diagnose schizophrenia just using it?
- Identify the best combination of acoustic features to diagnose schizophrenia using logistic regression.
- Discuss the "classification" process: which methods are you using? Which confounds should you be aware of? What are the strength and limitation of the analysis?
- Bonus question: Logistic regression is only one of many classification algorithms. Try using others and compare performance. Some examples: Discriminant Function, Random Forest, Support Vector Machine, etc. The package caret provides them. 
- Bonus Bonus question: *It is possible combine the output of multiple  classification models to improve classification accuracy. For inspiration see,*
https://machinelearningmastery.com/machine-learning-ensembles-with-r/
 The interested reader might also want to look up '*The BigChaos Solution to the Netflix Grand Prize*'

## Learning objectives
- Learn the basics of classification in a machine learning framework
- Design, fit and report logistic regressions
- Apply feature selection techniques


# Libraries
```{r}
pacman::p_load(tidyverse,lme4,tidyverse, purrr,dplyr, caret,e1071,pROC,readr,lmerTest,Metrics,merTools,tidymodels)
```

### Let's start

We first want to build a logistic regression to see whether you can diagnose schizophrenia from your best acoustic feature. Let's use the full dataset and calculate the different performance measures (accuracy, sensitivity, specificity, PPV, NPV, ROC curve). You need to think carefully as to how we should (or not) use study and subject ID.

```{r}
# Read data
df <- read.csv("npause_fixed.csv")

# Making logit 2 prob function
logit2prob <- function(logit){
  odds <- exp(logit)
  prob <- odds/(1+odds)
  return(prob)
}

# Renaming/reclassing because reasons 
df$Diagnosis <- as.factor(df$Diagnosis.y)
df$Study <- as.factor(df$Study.y)
df$Participant <- as.character(df$Participant)

```

# Running the same models again from last time
```{r}
# Acoustic feature: SpeechRate
m_sr <-
  glmer(
    Diagnosis ~ SpeechRate_scaled + (1 | Participant),
    data = df,
    family = "binomial")

# Acoustic feature: Pitch
m_pitch <-
  glmer(
    Diagnosis ~ IQR_scaled + (1 | Participant),
    data = df,
    family = "binomial")

# Including both acoustic features
# m_sr_pitch <-
#   glmer(
#     Diagnosis ~ IQR_scaled + SpeechRate_scaled + (1 | Participant),
#     data = df,
#     family = "binomial", control = glmerControl(optimizer = "nloptwrap", calc.derivs = FALSE))

# Checking anova
anova(m_speechrate,m_pitch)


```

The best model is m_pitch, so we will go with this feature. 

```{r}
# Making predicted values
df$pred <- logit2prob(predict(m_pitch))
df$pred <- as.numeric(df$pred)

# Predicted values from the model, and applying function to get the probabilities
df$pred[df$pred > 0.5] = "Schizophrenia"
df$pred[as.numeric(df$pred) <= 0.5] = "Control"
df$pred <- as.factor(df$pred)

# Making confusionMatrix
confusionMatrix(data = df$pred, reference = df$Diagnosis, positive = "Schizophrenia")

```

# Making a ROC curve
```{r}
# Making it numeric
df$pred <- as.numeric(df$pred)

rocCurve <- roc(response = df$Diagnosis, predictor = df$pred)
a <- as.numeric(auc(rocCurve)) #amazing #numeric
ci(rocCurve)
plot(rocCurve, legacy.axes = TRUE) + title("such results very help such education many learn") #LOOOOOOOOOOOOOOOOOL

```

Then cross-validate the logistic regression and re-calculate performance on the testing folds. N.B. The cross-validation functions you already have should be tweaked: you need to calculate these new performance measures. Alternatively, the groupdata2 and cvms package created by Ludvig are an easy solution. 

# Creating test/train set
```{r}
folding <- function(syntax){
  k = 10 
  folds <- createFolds(unique(df$Participant), k = k, list = TRUE, returnTrain = FALSE) 
  trainRMSE <- rep(NA, k) 
  testRMSE <- rep(NA, k) 
  
  train_auc = rep(NA, k)
  test_auc = rep(NA, k) 
  
  train_sens = rep(NA, k)
  test_sens = rep(NA, k)
  
  train_spec = rep(NA, k) #placeholders, fydler edm ud i nÃ¦ste loop
  test_spec = rep(NA, k) 
  
  #train_ci = rep(NA,k)
  #test_ci = rep(NA,k)
  
  train_PPV = rep(NA, k)
  test_PPV = rep(NA, k)
  train_NPV = rep(NA, k)
  test_NPV = rep(NA, k)

i = 1 
for (fold in folds) {
  train = subset(df,!(Participant %in% fold)) 
  test = subset(df, Participant %in% fold) 
  model = glmer(model, train, family = "binomial") 
  test <- test %>%
  mutate(prediction = logit2prob(predict(model, newdata = test,allow.new.levels = TRUE)),
         pred = as.factor(ifelse(prediction > .5, "Schizophrenia", "Control")))
  train <- train %>%
  mutate(prediction = logit2prob(predict(model)),
         pred = as.factor(ifelse(prediction > .5, "Schizophrenia", "Control")))
  
  trainRMSE[i] = rmse(train$CHI_MLU, fitted(model)) # saving RMSE value for the training data at index i
  testRMSE[i] = rmse(test$CHI_MLU, test$prediction) # saving RMSE value for the test data at index i
  train_rocCurve = roc(response = train$Diagnosis, predictor = as.numeric(train$pred))
  test_rocCurve = roc(response = test$Diagnosis, predictor = as.numeric(test$pred))
  train_auc[i] = as.numeric(auc(train_rocCurve))
  test_auc[i] = as.numeric(auc(test_rocCurve))
  train_sens[i] = sensitivity(data = train$pred, reference = train$Diagnosis, positive = "Schizophrenia")
  test_sens[i] = sensitivity(data = test$pred, reference = test$Diagnosis, positive = "Schizophrenia")
  train_spec[i] = specificity(data = train$pred, reference = train$Diagnosis, negative = "Control")
  test_spec[i] = specificity(data = test$pred, reference = test$Diagnosis, negative = "Control")
  #train_ci[i] = ci(train_rocCurve)
  #test_ci[i] = ci(test_rocCurve)
  train_PPV[i] = posPredValue(data = train$pred, reference = train$Diagnosis, positive = "Schizophrenia")
  test_PPV[i] = posPredValue(data = test$pred, reference = test$Diagnosis, positive = "Schizophrenia")
  train_NPV[i] = negPredValue(data = train$pred, reference = train$Diagnosis, negative = "Control")
  test_NPV[i] = negPredValue(data = test$pred, reference = test$Diagnosis, negative = "Control")

  i = i + 1
}
  
  m_train_auc = mean(train_auc)
  m_test_auc = mean(test_auc)
  m_train_sens = mean(train_sens)
  m_test_sens = mean(test_sens)
  m_train_spec = mean(train_spec)
  m_test_spec = mean(test_spec)

  m_train_RMSE = mean(trainRMSE)
  m_test_RMSE = mean(testRMSE) 
  DiffTrainTest = m_train - m_test

  return(c(m_train_auc, m_test_auc,m_train_sens,m_test_sens, m_train_spec, m_test_spec,m_train, m_test, DiffTrainTest)) # specifying what we want the function to return
}

```


```{r}
# The syntax of our two models, where m_pitch was the best one
m_sr
m_pitch

# Storing the syntax and names of our models so we can loop through them
models <- c(m_sr, m_pitch)
names <- c('m_sr', 'm_pitch')

# Creating place holder df for the loop
obj <- matrix(0, ncol = 11, nrow = 10) %>% data.frame()

# Changing the col-names in our place holder df
colnames(obj)[1] <- 'ModelNames'
colnames(obj)[2] <- 'm_train_auc'
colnames(obj)[3] <- 'm_test_auc'
colnames(obj)[4] <- 'm_train_sens'
colnames(obj)[5] <- 'm_test_sens'
colnames(obj)[6] <- 'm_train_spec'
colnames(obj)[7] <- 'm_test_spec'
colnames(obj)[8] <- 'm_test_sens'
colnames(obj)[9] <- 'm_train_RMSE'
colnames(obj)[10] <- 'm_test_RMSE'
colnames(obj)[11] <- 'DiffTrainTest'


# A loop
i = 1 # indexing at i
for (model in models) {
  obj[i, 1] <- names[i] # saving the name of the models
  obj[i, 2:11] <- folding(model) # using the folding function 
  i = i + 1 # increasing index by 1
}
```

N.B. the predict() function generates log odds (the full scale between minus and plus infinity). Log odds > 0 indicates a choice of 1, below a choice of 0.
N.N.B. you need to decide whether calculate performance on each single test fold or save all the prediction for test folds in one datase, so to calculate overall performance.
N.N.N.B. Now you have two levels of structure: subject and study. Should this impact your cross-validation?
N.N.N.N.B. A more advanced solution could rely on the tidymodels set of packages (warning: Time-consuming to learn as the documentation is sparse, but totally worth it)


```{r}
# Data cleaning
library(tidyverse)
read_pitch <- function(filename) {
  #read data
  d <-
    read_delim(
      paste0(
        "/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/Pitch/",
        filename
      ),
      delim = "\t"
    )
  #parse filename; study, diagnosis, subject, trial
  vars = str_match(filename, "Study(\\d+)D([01])S(\\d+)T(\\d+)")
  vars = as.data.frame(t(vars[2:length(vars)]))
  names(vars) = c("study", "diagnosis", "subject", "trial")
  #extract descriptors
  mean <- mean(d$f0)
  sd <- sd(d$f0)
  min <- min(d$f0)
  max <- max(d$f0)
  median <- median(d$f0)
  iqr <- IQR(d$f0) #InterQuartile Range
  mad <- mad(d$f0) #Median absolute deviation
  range <- max(d$f0) -  min(d$f0)
  coefvar <- sd(d$f0) / mean(d$f0) #Coefficient variation
  d <-
    cbind(vars, data.frame(mean, sd, min, max, median, iqr, mad, coefvar))
  #combine all this data
  return(d)
}
pitch_data = list.files(path = "/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/Pitch/", pattern = ".txt") %>%
  purrr::map_df(read_pitch)
write_csv(
  pitch_data,
  "/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/Pitch/pitch_data.csv"
)
# Let's start with the demographic and clinical data
Demo <-
  read_delim(
    "/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/DemographicData.csv",
    delim = ";"
  ) %>%
  rename(ID = Participant) %>%
  mutate(ID = factor(ID),
         Study = factor(Study))

# then duration data
Duration <-
  read_delim(
    "/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/Articulation.txt",
    delim = ","
  )
vars = str_match(Duration$soundname, "Study(\\d+)D([01])S(\\d+)T(\\d+)")
Duration <- Duration %>% mutate(
  Study = factor(vars[, 2]),
  Diagnosis = ifelse(vars[, 3] == 0, "Control", "Schizophrenia"),
  ID = factor(as.numeric(vars[, 4])),
  Trial = vars[, 5],
  PauseDuration = (as.numeric(` dur (s)`) - as.numeric(` phonationtime (s)`)) /
    as.numeric(` npause`)
) %>%
  rename(
    Study = Study,
    Diagnosis = Diagnosis,
    ID = ID,
    Trial = Trial,
    SyllableN = ` nsyll`,
    PauseN = ` npause`,
    Duration = ` dur (s)`,
    SpokenDuration = ` phonationtime (s)`,
    SpeechRate = ` speechrate (nsyll/dur)`,
    ArticulationRate = ` articulation rate (nsyll / phonationtime)`,
    SyllableDuration = ` ASD (speakingtime/nsyll)`,
    PauseDuration = PauseDuration
  )
Duration$PauseDuration[!is.finite(Duration$PauseDuration)] <-
  NA # or 0
Pitch <-
  read_csv(
    "/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/pitch_data.csv"
  ) %>%
  rename(
    ID = subject,
    Study = study,
    Diagnosis = diagnosis,
    Trial = trial
  ) %>%
  mutate(
    ID = factor(ID),
    Study = factor(Study),
    Diagnosis = factor(ifelse(Diagnosis == 0, "Control", "Schizophrenia"))
  )
# Now we merge them
d <- merge(Pitch, Duration, all = T)
d <- merge(d, Demo, all = T)
d <- d %>% subset(!is.na(Trial))
# Now we save them
write_csv(d,"/Users/au598019/Dropbox/Teaching/2020 methods 3/Assignments20/Assignment3/data/data_merge.csv")

```

```{r}

```












